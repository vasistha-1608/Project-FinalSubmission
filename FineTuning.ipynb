{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5a80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabular[all]\n",
      "  Downloading pytorch_tabular-1.1.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.4.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (1.3.2)\n",
      "Collecting pytorch-lightning<2.5.0,>=2.0.0 (from pytorch-tabular[all])\n",
      "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting omegaconf>=2.3.0 (from pytorch-tabular[all])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting torchmetrics<1.6.0,>=0.10.0 (from pytorch-tabular[all])\n",
      "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>2.2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.13.0)\n",
      "Requirement already satisfied: protobuf<5.29.0,>=3.20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (4.25.6)\n",
      "Collecting pytorch-tabnet==4.1 (from pytorch-tabular[all])\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (6.0.1)\n",
      "Requirement already satisfied: matplotlib>3.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (3.4.2)\n",
      "Collecting ipywidgets (from pytorch-tabular[all])\n",
      "  Downloading ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting einops<0.8.0,>=0.6.0 (from pytorch-tabular[all])\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rich>=11.0.0 (from pytorch-tabular[all])\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabnet==4.1->pytorch-tabular[all]) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabnet==4.1->pytorch-tabular[all]) (4.66.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (2.9.0.post0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.3.0->pytorch-tabular[all])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas>=1.1.5->pytorch-tabular[all]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas>=1.1.5->pytorch-tabular[all]) (2024.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (4.5.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.0.0->pytorch-tabular[all])\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from rich>=11.0.0->pytorch-tabular[all]) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn>=1.3.0->pytorch-tabular[all]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn>=1.3.0->pytorch-tabular[all]) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (72.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.0.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.43.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.16.1)\n",
      "Collecting typing-extensions>=4.4.0 (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.1.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets->pytorch-tabular[all])\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.14 (from ipywidgets->pytorch-tabular[all])\n",
      "  Downloading jupyterlab_widgets-3.0.14-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading aiohttp-3.10.11-cp38-cp38-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.0.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (7.0.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch-tabular[all])\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>3.1->pytorch-tabular[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from sympy->torch>=1.11.0->pytorch-tabular[all]) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading frozenlist-1.5.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading multidict-6.1.0-cp38-cp38-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading yarl-1.15.2-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.2)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Downloading propcache-0.2.0-cp38-cp38-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "   ---------------------------------------- 0.0/815.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 815.2/815.2 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
      "   ---------------------------------------- 0.0/891.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 891.4/891.4 kB 19.7 MB/s eta 0:00:00\n",
      "Downloading ipywidgets-8.1.6-py3-none-any.whl (139 kB)\n",
      "Downloading pytorch_tabular-1.1.1-py2.py3-none-any.whl (163 kB)\n",
      "Downloading jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\n",
      "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 41.2 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.10.11-cp38-cp38-win_amd64.whl (384 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp38-cp38-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.15.2-cp38-cp38-win_amd64.whl (84 kB)\n",
      "Downloading propcache-0.2.0-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144577 sha256=4f4fa7e1641c4a2c4acc9758a84ae531f78f018244cb38f68bff1a88f4f0cdd0\n",
      "  Stored in directory: c:\\users\\vasistha\\appdata\\local\\pip\\cache\\wheels\\b1\\a3\\c2\\6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, widgetsnbextension, typing-extensions, propcache, omegaconf, mdurl, jupyterlab_widgets, frozenlist, einops, async-timeout, aiohappyeyeballs, multidict, markdown-it-py, lightning-utilities, aiosignal, yarl, torchmetrics, rich, pytorch-tabnet, ipywidgets, aiohttp, pytorch-lightning, pytorch-tabular\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-5.0.1 einops-0.7.0 frozenlist-1.5.0 ipywidgets-8.1.6 jupyterlab_widgets-3.0.14 lightning-utilities-0.11.9 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 omegaconf-2.3.0 propcache-0.2.0 pytorch-lightning-2.4.0 pytorch-tabnet-4.1.0 pytorch-tabular-1.1.1 rich-14.0.0 torchmetrics-1.5.2 typing-extensions-4.13.2 widgetsnbextension-4.0.14 yarl-1.15.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pytorch-tabular 1.1.1 does not provide the extra 'all'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabular[all]\n",
      "  Using cached pytorch_tabular-1.1.1-py2.py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.4.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (1.3.2)\n",
      "Collecting pytorch-lightning<2.5.0,>=2.0.0 (from pytorch-tabular[all])\n",
      "  Using cached pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting omegaconf>=2.3.0 (from pytorch-tabular[all])\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting torchmetrics<1.6.0,>=0.10.0 (from pytorch-tabular[all])\n",
      "  Using cached torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>2.2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (2.13.0)\n",
      "Requirement already satisfied: protobuf<5.29.0,>=3.20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (4.25.6)\n",
      "Collecting pytorch-tabnet==4.1 (from pytorch-tabular[all])\n",
      "  Using cached pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (6.0.1)\n",
      "Requirement already satisfied: matplotlib>3.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabular[all]) (3.4.2)\n",
      "Collecting ipywidgets (from pytorch-tabular[all])\n",
      "  Using cached ipywidgets-8.1.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting einops<0.8.0,>=0.6.0 (from pytorch-tabular[all])\n",
      "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting rich>=11.0.0 (from pytorch-tabular[all])\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabnet==4.1->pytorch-tabular[all]) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-tabnet==4.1->pytorch-tabular[all]) (4.66.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib>3.1->pytorch-tabular[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from omegaconf>=2.3.0->pytorch-tabular[all]) (4.9.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas>=1.1.5->pytorch-tabular[all]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pandas>=1.1.5->pytorch-tabular[all]) (2024.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (4.5.0)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.0.0->pytorch-tabular[all])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from rich>=11.0.0->pytorch-tabular[all]) (2.15.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn>=1.3.0->pytorch-tabular[all]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from scikit-learn>=1.3.0->pytorch-tabular[all]) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (1.70.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (72.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.0.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.43.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.16.1)\n",
      "Collecting typing-extensions>=4.4.0 (from pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.11.0->pytorch-tabular[all]) (3.1.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipywidgets->pytorch-tabular[all]) (4.0.14)\n",
      "Collecting jupyterlab_widgets~=3.0.14 (from ipywidgets->pytorch-tabular[all])\n",
      "  Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached aiohttp-3.10.11-cp38-cp38-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.0.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (7.0.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch-tabular[all])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>3.1->pytorch-tabular[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from sympy->torch>=1.11.0->pytorch-tabular[all]) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all]) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached frozenlist-1.5.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached multidict-6.1.0-cp38-cp38-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached yarl-1.15.2-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.5.0,>2.2.0->pytorch-tabular[all]) (3.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->pytorch-tabular[all]) (0.2.2)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch-tabular[all])\n",
      "  Using cached propcache-0.2.0-cp38-cp38-win_amd64.whl.metadata (7.9 kB)\n",
      "Using cached pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
      "Using cached ipywidgets-8.1.6-py3-none-any.whl (139 kB)\n",
      "Using cached pytorch_tabular-1.1.1-py2.py3-none-any.whl (163 kB)\n",
      "Using cached jupyterlab_widgets-3.0.14-py3-none-any.whl (213 kB)\n",
      "Using cached lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached aiohttp-3.10.11-cp38-cp38-win_amd64.whl (384 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached frozenlist-1.5.0-cp38-cp38-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Using cached yarl-1.15.2-cp38-cp38-win_amd64.whl (84 kB)\n",
      "Using cached propcache-0.2.0-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Installing collected packages: typing-extensions, propcache, omegaconf, mdurl, jupyterlab_widgets, frozenlist, einops, async-timeout, aiohappyeyeballs, multidict, markdown-it-py, lightning-utilities, aiosignal, yarl, torchmetrics, rich, pytorch-tabnet, ipywidgets, aiohttp, pytorch-lightning, pytorch-tabular\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 async-timeout-5.0.1 einops-0.7.0 frozenlist-1.5.0 ipywidgets-8.1.6 jupyterlab_widgets-3.0.14 lightning-utilities-0.11.9 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 omegaconf-2.3.0 propcache-0.2.0 pytorch-lightning-2.4.0 pytorch-tabnet-4.1.0 pytorch-tabular-1.1.1 rich-14.0.0 torchmetrics-1.5.2 typing-extensions-4.13.2 yarl-1.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pytorch-tabular 1.1.1 does not provide the extra 'all'\n",
      "WARNING: Error parsing dependencies of typing-extensions: [Errno 2] No such file or directory: 'c:\\\\users\\\\vasistha\\\\anaconda3\\\\envs\\\\textmining\\\\lib\\\\site-packages\\\\typing_extensions-4.5.0.dist-info\\\\METADATA'\n",
      "    WARNING: No metadata found in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.13.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-tabular[all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f08c1193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">691</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:14\u001b[0m,\u001b[1;36m691\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">741</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:14\u001b[0m,\u001b[1;36m741\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">760</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:14\u001b[0m,\u001b[1;36m760\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "classification task                                                                                                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">857</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:14\u001b[0m,\u001b[1;36m857\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">986</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:14\u001b[0m,\u001b[1;36m986\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:15</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">042</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:15\u001b[0m,\u001b[1;36m042\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\vasistha\\Human-Centered-DataScience\\Project\\saved_models exists and is not empty.\n",
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a971655c89c4b84897479c8a6cc31fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 1.3182567385564074e-07\n",
      "Restoring states from the checkpoint path at c:\\Users\\vasistha\\Human-Centered-DataScience\\Project\\.lr_find_3f69a336-bb17-4b94-838b-3f1f1ba5f10b.ckpt\n",
      "Restored all states from the checkpoint at c:\\Users\\vasistha\\Human-Centered-DataScience\\Project\\.lr_find_3f69a336-bb17-4b94-838b-3f1f1ba5f10b.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">914</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3182567385564074e-07</span>. For   \n",
       "plot and detailed analysis, use `find_learning_rate` method.                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:20\u001b[0m,\u001b[1;36m914\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m1.3182567385564074e-07\u001b[0m. For   \n",
       "plot and detailed analysis, use `find_learning_rate` method.                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:25:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">933</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:25:20\u001b[0m,\u001b[1;36m933\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ TabTransformerBackbone │  271 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding2dLayer       │  4.9 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _head            │ LinearHead             │    240 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ CrossEntropyLoss       │      0 │ train │\n",
       "└───┴──────────────────┴────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabTransformerBackbone │  271 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding2dLayer       │  4.9 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _head            │ LinearHead             │    240 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ CrossEntropyLoss       │      0 │ train │\n",
       "└───┴──────────────────┴────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 276 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 276 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 123                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 276 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 276 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 123                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea7c7ede3b8403f9bf760c520d92fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:26:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">753</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:26:06\u001b[0m,\u001b[1;36m753\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:26:06</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">753</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:26:06\u001b[0m,\u001b[1;36m753\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b4aa56d73b46248e5a1267fed159ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_tabular\\utils\\python_utils.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n",
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4789067208766937     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7823614478111267     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_loss_0        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7823614478111267     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4789067208766937    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7823614478111267    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_loss_0       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7823614478111267    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test_loss_0': 0.7823614478111267, 'test_loss': 0.7823614478111267, 'test_accuracy': 0.4789067208766937}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models.tab_transformer.config import TabTransformerConfig\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "\n",
    "# 1. Load your dataset\n",
    "df = pd.read_csv(\"cleaned_merged_stock_topic_data_3.csv\")\n",
    "\n",
    "# 2. Preprocessing\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Date\"])\n",
    "df[\"Direction\"] = df[\"Direction\"].map({\"Up\": 1, \"Down\": 0})  # Convert target to numeric\n",
    "\n",
    "# 3. Define Configs\n",
    "target_col = \"Direction\"\n",
    "categorical_cols = [\"Stock\",\"Date\"]  # Add other categorical columns if needed\n",
    "continuous_cols = [col for col in df.columns if col not in [\"Date\", \"Direction\", \"Stock\",\"Unnamed: 0\",'Daily_Return']]\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=[\"Direction\"],  # <-- make this a list, not a string\n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    normalize_continuous_features=True\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    batch_size=64,\n",
    "    max_epochs=25,\n",
    "    early_stopping=\"valid_loss\",\n",
    "    checkpoints=\"valid_loss\",   \n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "\n",
    "model_config = TabTransformerConfig(\n",
    "    task=\"classification\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    learning_rate=1e-4,\n",
    "    input_embed_dim=32,               # Size of categorical embeddings\n",
    "    share_embedding=True,             # Use shared embeddings for categorical features\n",
    "    share_embedding_strategy=\"fraction\",  # Strategy to share embedding space\n",
    "    shared_embedding_fraction=0.25,   # How much of embedding is shared\n",
    "    num_heads=8,                      # Number of attention heads\n",
    "    num_attn_blocks=6,                # Number of attention blocks (layers)\n",
    "    attn_dropout=0.1,                 # Dropout after attention\n",
    "    add_norm_dropout=0.1,             # Dropout in Add-Norm\n",
    "    ff_dropout=0.1,                   # Dropout in FeedForward layers\n",
    "    ff_hidden_multiplier=4,          # FFN size = 4x embedding size\n",
    "    transformer_activation=\"GEGLU\",   # Activation in transformer FFN\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Initialize and Train Model\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "tabular_model.fit(train=df)\n",
    "\n",
    "# 5. Evaluate\n",
    "results = tabular_model.evaluate(df)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb44a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TabTransformerConfig in module pytorch_tabular.models.tab_transformer.config:\n",
      "\n",
      "class TabTransformerConfig(pytorch_tabular.config.config.ModelConfig)\n",
      " |  TabTransformerConfig(task: str, head: Union[str, NoneType] = 'LinearHead', head_config: Union[Dict, NoneType] = <factory>, embedding_dims: Union[List, NoneType] = None, embedding_dropout: float = 0.0, batch_norm_continuous_input: bool = True, learning_rate: float = 0.001, loss: Union[str, NoneType] = None, metrics: Union[List[str], NoneType] = None, metrics_prob_input: Union[List[bool], NoneType] = None, metrics_params: Union[List, NoneType] = None, target_range: Union[List, NoneType] = None, virtual_batch_size: Union[int, NoneType] = None, seed: int = 42, _module_src: str = 'models.tab_transformer', _model_name: str = 'TabTransformerModel', _backbone_name: str = 'TabTransformerBackbone', _config_name: str = 'TabTransformerConfig', input_embed_dim: int = 32, embedding_initialization: Union[str, NoneType] = 'kaiming_uniform', embedding_bias: bool = False, share_embedding: bool = False, share_embedding_strategy: Union[str, NoneType] = 'fraction', shared_embedding_fraction: float = 0.25, num_heads: int = 8, num_attn_blocks: int = 6, transformer_head_dim: Union[int, NoneType] = None, attn_dropout: float = 0.1, add_norm_dropout: float = 0.1, ff_dropout: float = 0.1, ff_hidden_multiplier: int = 4, transformer_activation: str = 'GEGLU') -> None\n",
      " |  \n",
      " |  Tab Transformer configuration.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_embed_dim (int): The embedding dimension for the input categorical features. Defaults to 32\n",
      " |  \n",
      " |      embedding_initialization (Optional[str]): Initialization scheme for the embedding layers. Defaults\n",
      " |              to `kaiming`. Choices are: [`kaiming_uniform`,`kaiming_normal`].\n",
      " |  \n",
      " |      embedding_bias (bool): Flag to turn on Embedding Bias. Defaults to False\n",
      " |  \n",
      " |      share_embedding (bool): The flag turns on shared embeddings in the input embedding process. The key\n",
      " |              idea here is to have an embedding for the feature as a whole along with embeddings of each unique\n",
      " |              values of that column. For more details refer to Appendix A of the TabTransformer paper. Defaults\n",
      " |              to False\n",
      " |  \n",
      " |      share_embedding_strategy (Optional[str]): There are two strategies in adding shared embeddings. 1.\n",
      " |              `add` - A separate embedding for the feature is added to the embedding of the unique values of the\n",
      " |              feature. 2. `fraction` - A fraction of the input embedding is reserved for the shared embedding of\n",
      " |              the feature. Defaults to fraction. Choices are: [`add`,`fraction`].\n",
      " |  \n",
      " |      shared_embedding_fraction (float): Fraction of the input_embed_dim to be reserved by the shared\n",
      " |              embedding. Should be less than one. Defaults to 0.25\n",
      " |  \n",
      " |      num_heads (int): The number of heads in the Multi-Headed Attention layer. Defaults to 8\n",
      " |  \n",
      " |      num_attn_blocks (int): The number of layers of stacked Multi-Headed Attention layers. Defaults to 6\n",
      " |  \n",
      " |      transformer_head_dim (Optional[int]): The number of hidden units in the Multi-Headed Attention\n",
      " |              layers. Defaults to None and will be same as input_dim.\n",
      " |  \n",
      " |      attn_dropout (float): Dropout to be applied after Multi headed Attention. Defaults to 0.1\n",
      " |  \n",
      " |      add_norm_dropout (float): Dropout to be applied in the AddNorm Layer. Defaults to 0.1\n",
      " |  \n",
      " |      ff_dropout (float): Dropout to be applied in the Positionwise FeedForward Network. Defaults to 0.1\n",
      " |  \n",
      " |      ff_hidden_multiplier (int): Multiple by which the Positionwise FF layer scales the input. Defaults\n",
      " |              to 4\n",
      " |  \n",
      " |      transformer_activation (str): The activation type in the transformer feed forward layers. In\n",
      " |              addition to the default activation in PyTorch like ReLU, TanH, LeakyReLU, etc.\n",
      " |              https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity, GEGLU,\n",
      " |              ReGLU and SwiGLU are also implemented(https://arxiv.org/pdf/2002.05202.pdf). Defaults to GEGLU\n",
      " |  \n",
      " |      task (str): Specify whether the problem is regression or classification. `backbone` is a task which\n",
      " |              considers the model as a backbone to generate features. Mostly used internally for SSL and related\n",
      " |              tasks.. Choices are: [`regression`,`classification`,`backbone`].\n",
      " |  \n",
      " |      head (Optional[str]): The head to be used for the model. Should be one of the heads defined in\n",
      " |              `pytorch_tabular.models.common.heads`. Defaults to  LinearHead. Choices are:\n",
      " |              [`None`,`LinearHead`,`MixtureDensityHead`].\n",
      " |  \n",
      " |      head_config (Optional[Dict]): The config as a dict which defines the head. If left empty, will be\n",
      " |              initialized as default linear head.\n",
      " |  \n",
      " |      embedding_dims (Optional[List]): The dimensions of the embedding for each categorical column as a\n",
      " |              list of tuples (cardinality, embedding_dim). If left empty, will infer using the cardinality of\n",
      " |              the categorical column using the rule min(50, (x + 1) // 2)\n",
      " |  \n",
      " |      embedding_dropout (float): Dropout to be applied to the Categorical Embedding. Defaults to 0.0\n",
      " |  \n",
      " |      batch_norm_continuous_input (bool): If True, we will normalize the continuous layer by passing it\n",
      " |              through a BatchNorm layer.\n",
      " |  \n",
      " |      learning_rate (float): The learning rate of the model. Defaults to 1e-3.\n",
      " |  \n",
      " |      loss (Optional[str]): The loss function to be applied. By Default, it is MSELoss for regression and\n",
      " |              CrossEntropyLoss for classification. Unless you are sure what you are doing, leave it at MSELoss\n",
      " |              or L1Loss for regression and CrossEntropyLoss for classification\n",
      " |  \n",
      " |      metrics (Optional[List[str]]): the list of metrics you need to track during training. The metrics\n",
      " |              should be one of the functional metrics implemented in ``torchmetrics``. By default, it is\n",
      " |              accuracy if classification and mean_squared_error for regression\n",
      " |  \n",
      " |      metrics_params (Optional[List]): The parameters to be passed to the metrics function. `task` is forced to\n",
      " |              be `multiclass` because the multiclass version can handle binary as well and for simplicity we are\n",
      " |              only using `multiclass`.\n",
      " |  \n",
      " |      metrics_prob_input (Optional[List]): Is a mandatory parameter for classification metrics defined in the config.\n",
      " |          This defines whether the input to the metric function is the probability or the class. Length should be\n",
      " |          same as the number of metrics. Defaults to None.\n",
      " |  \n",
      " |      target_range (Optional[List]): The range in which we should limit the output variable. Currently\n",
      " |              ignored for multi-target regression. Typically used for Regression problems. If left empty, will\n",
      " |              not apply any restrictions\n",
      " |  \n",
      " |      seed (int): The seed for reproducibility. Defaults to 42\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TabTransformerConfig\n",
      " |      pytorch_tabular.config.config.ModelConfig\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__(self, task: str, head: Union[str, NoneType] = 'LinearHead', head_config: Union[Dict, NoneType] = <factory>, embedding_dims: Union[List, NoneType] = None, embedding_dropout: float = 0.0, batch_norm_continuous_input: bool = True, learning_rate: float = 0.001, loss: Union[str, NoneType] = None, metrics: Union[List[str], NoneType] = None, metrics_prob_input: Union[List[bool], NoneType] = None, metrics_params: Union[List, NoneType] = None, target_range: Union[List, NoneType] = None, virtual_batch_size: Union[int, NoneType] = None, seed: int = 42, _module_src: str = 'models.tab_transformer', _model_name: str = 'TabTransformerModel', _backbone_name: str = 'TabTransformerBackbone', _config_name: str = 'TabTransformerConfig', input_embed_dim: int = 32, embedding_initialization: Union[str, NoneType] = 'kaiming_uniform', embedding_bias: bool = False, share_embedding: bool = False, share_embedding_strategy: Union[str, NoneType] = 'fraction', shared_embedding_fraction: float = 0.25, num_heads: int = 8, num_attn_blocks: int = 6, transformer_head_dim: Union[int, NoneType] = None, attn_dropout: float = 0.1, add_norm_dropout: float = 0.1, ff_dropout: float = 0.1, ff_hidden_multiplier: int = 4, transformer_activation: str = 'GEGLU') -> None\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_backbone_name': <class 'str'>, '_config_name': <c...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'_backbone_name': Field(name='_backbone_name',...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  add_norm_dropout = 0.1\n",
      " |  \n",
      " |  attn_dropout = 0.1\n",
      " |  \n",
      " |  embedding_bias = False\n",
      " |  \n",
      " |  embedding_initialization = 'kaiming_uniform'\n",
      " |  \n",
      " |  ff_dropout = 0.1\n",
      " |  \n",
      " |  ff_hidden_multiplier = 4\n",
      " |  \n",
      " |  input_embed_dim = 32\n",
      " |  \n",
      " |  num_attn_blocks = 6\n",
      " |  \n",
      " |  num_heads = 8\n",
      " |  \n",
      " |  share_embedding = False\n",
      " |  \n",
      " |  share_embedding_strategy = 'fraction'\n",
      " |  \n",
      " |  shared_embedding_fraction = 0.25\n",
      " |  \n",
      " |  transformer_activation = 'GEGLU'\n",
      " |  \n",
      " |  transformer_head_dim = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_tabular.config.config.ModelConfig:\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_tabular.config.config.ModelConfig:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_tabular.config.config.ModelConfig:\n",
      " |  \n",
      " |  batch_norm_continuous_input = True\n",
      " |  \n",
      " |  embedding_dims = None\n",
      " |  \n",
      " |  embedding_dropout = 0.0\n",
      " |  \n",
      " |  head = 'LinearHead'\n",
      " |  \n",
      " |  learning_rate = 0.001\n",
      " |  \n",
      " |  loss = None\n",
      " |  \n",
      " |  metrics = None\n",
      " |  \n",
      " |  metrics_params = None\n",
      " |  \n",
      " |  metrics_prob_input = None\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  target_range = None\n",
      " |  \n",
      " |  virtual_batch_size = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular.models.tab_transformer.config import TabTransformerConfig\n",
    "help(TabTransformerConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebaf2980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from captum) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from captum) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.6 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from captum) (2.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from captum) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from torch>=1.6->captum) (2024.10.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib->captum) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib->captum) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a40520",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Attributions are not implemented for TabTransformerModel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m explanations \u001b[38;5;241m=\u001b[39m \u001b[43mtabular_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIntegratedGradients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(explanations\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_tabular\\tabular_model.py:2051\u001b[0m, in \u001b[0;36mTabularModel.explain\u001b[1;34m(self, data, method, method_args, baselines, **kwargs)\u001b[0m\n\u001b[0;32m   2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaselines cannot be a scalar or None for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a tensor or a string like `b|<num_samples>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2049\u001b[0m     )\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_not_supported:\n\u001b[1;32m-> 2051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributions are not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_get_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2053\u001b[0m is_embedding1d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membedding_layer, (Embedding1dLayer, PreEncoded1dLayer))\n\u001b[0;32m   2054\u001b[0m is_embedding2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membedding_layer, Embedding2dLayer)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Attributions are not implemented for TabTransformerModel"
     ]
    }
   ],
   "source": [
    "explanations = tabular_model.explain(df, method=\"IntegratedGradients\")\n",
    "print(explanations.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a62561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model.save_model(\"my_finetuned_tabtransformer/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c2000b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:27:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">475</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:27:23\u001b[0m,\u001b[1;36m475\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m171\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">04</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">21:27:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">492</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m04\u001b[0m-\u001b[1;36m21\u001b[0m \u001b[1;92m21:27:23\u001b[0m,\u001b[1;36m492\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0364cb7277b402f8dca826bf5144a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.45543670654296875    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7794811725616455     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_loss_0        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7794811725616455     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.45543670654296875   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7794811725616455    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_loss_0       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7794811725616455    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation Results:\n",
      " [{'test_loss_0': 0.7794811725616455, 'test_loss': 0.7794811725616455, 'test_accuracy': 0.45543670654296875}]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.65      0.56       602\n",
      "           1       0.36      0.23      0.28       520\n",
      "\n",
      "    accuracy                           0.46      1122\n",
      "   macro avg       0.43      0.44      0.42      1122\n",
      "weighted avg       0.43      0.46      0.43      1122\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "from pytorch_tabular import TabularModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "# from pytorch_tabular.models import load_model\n",
    "# 2. Load your test dataset\n",
    "# Replace this with your actual test data path\n",
    "test_df = pd.read_csv(\"TestDataset/final_merged_stock_topic_sentiment_test.csv\")\n",
    "\n",
    "# OPTIONAL: convert 'Direction' to numeric if it's still 'Up'/'Down'\n",
    "# test_df[\"Direction\"] = test_df[\"Direction\"].map({\"Down\": 0, \"Up\": 1})\n",
    "\n",
    "# 2. Drop unnecessary columns\n",
    "\n",
    "\n",
    "# 3. If Direction is in string format ('Up'/'Down'), map it to numeric\n",
    "if test_df[\"Direction\"].dtype == object:\n",
    "    test_df[\"Direction\"] = test_df[\"Direction\"].map({\"Down\": 0, \"Up\": 1})\n",
    "    \n",
    "drop_cols = [\"Unnamed: 0.1\"]\n",
    "test_df = test_df.drop(columns=[col for col in drop_cols if col in test_df.columns])\n",
    "\n",
    "# Get all feature columns used in training\n",
    "all_required_columns = tabular_model.config.continuous_cols + tabular_model.config.categorical_cols\n",
    "\n",
    "# Add missing columns with default value 0\n",
    "for col in all_required_columns:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0  # Assume no signal for that topic/sentiment on that day\n",
    "\n",
    "# Ensure correct column order\n",
    "test_df = test_df[all_required_columns + tabular_model.config.target]\n",
    "\n",
    "\n",
    "# 3. Load the trained model\n",
    "model_path = \"my_finetuned_tabtransformer\"  # Folder where you saved it\n",
    "tabular_model = TabularModel.load_model(\"my_finetuned_tabtransformer\")\n",
    "\n",
    "\n",
    "# 4. Evaluate the model (prints loss, accuracy, etc.)\n",
    "results = tabular_model.evaluate(test_df)\n",
    "print(\"📊 Evaluation Results:\\n\", results)\n",
    "\n",
    "# 5. Predict the target values\n",
    "preds = tabular_model.predict(test_df)\n",
    "\n",
    "# 6. Attach predictions to test_df\n",
    "# Attach predictions to test_df\n",
    "test_df[\"Predicted\"] = preds[\"Direction_prediction\"]\n",
    "\n",
    "\n",
    "# 7. Print Classification Report & Confusion Matrix\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(test_df[\"Direction\"], test_df[\"Predicted\"]))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\")\n",
    "ConfusionMatrixDisplay.from_predictions(test_df[\"Direction\"], test_df[\"Predicted\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# # 8. (Optional) Save results\n",
    "# test_df.to_csv(\"test_predictions_with_actuals.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18423ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n"
     ]
    }
   ],
   "source": [
    "import pytorch_tabular\n",
    "print(pytorch_tabular.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2f786fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected categorical cols: ['Stock', 'Date']\n",
      "Expected continuous cols: ['Open', 'High', 'Low', 'Close', 'Volume', 'Daily_Return', 'Volatility_10', 'Volatility_20', 'SMA_10', 'SMA_20', 'std_10', 'std_20', 'Upper_BB_10', 'Lower_BB_10', 'Upper_BB_20', 'Lower_BB_20', 'topic_-1_negative', 'topic_-1_neutral', 'topic_-1_positive', 'topic_0_negative', 'topic_0_neutral', 'topic_0_positive', 'topic_1_negative', 'topic_1_neutral', 'topic_1_positive', 'topic_2_negative', 'topic_2_neutral', 'topic_2_positive', 'topic_3_negative', 'topic_3_neutral', 'topic_3_positive', 'topic_4_negative', 'topic_4_neutral', 'topic_4_positive', 'topic_5_negative', 'topic_5_neutral', 'topic_5_positive', 'topic_6_negative', 'topic_6_neutral', 'topic_6_positive', 'topic_7_negative', 'topic_7_neutral', 'topic_7_positive', 'topic_8_negative', 'topic_8_neutral', 'topic_8_positive', 'topic_-1', 'topic_0', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8']\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected categorical cols:\", tabular_model.config.categorical_cols)\n",
    "print(\"Expected continuous cols:\", tabular_model.config.continuous_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ffb9e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your test_df columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Daily_Return', 'Volatility_10', 'Volatility_20', 'SMA_10', 'SMA_20', 'std_10', 'std_20', 'Upper_BB_10', 'Lower_BB_10', 'Upper_BB_20', 'Lower_BB_20', 'Direction', 'Total Comments', 'Negative Comments', 'Positive Comments', 'Neutral Comments', 'Topic_-1', 'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3', 'Topic_4', 'Topic_5', 'Topic_6', 'Topic_7', 'Topic_8', 'Sentiment_Score', 'Sentiment_Intensity', 'Weighted_Sentiment', 'topic_-1_negative', 'topic_-1_neutral', 'topic_-1_positive', 'topic_0_negative', 'topic_0_neutral', 'topic_0_positive', 'topic_1_negative', 'topic_1_neutral', 'topic_1_positive', 'topic_2_negative', 'topic_2_neutral', 'topic_2_positive', 'topic_3_negative', 'topic_3_neutral', 'topic_3_positive', 'topic_4_negative', 'topic_4_neutral', 'topic_5_negative', 'topic_5_neutral', 'topic_5_positive', 'topic_6_negative', 'topic_6_neutral', 'topic_6_positive', 'topic_7_negative', 'topic_7_neutral', 'topic_7_positive', 'topic_8_negative', 'topic_8_neutral', 'topic_8_positive']\n"
     ]
    }
   ],
   "source": [
    "print(\"Your test_df columns:\", test_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60fa406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column  Stock  is missing\n",
      "The column  Date  is missing\n"
     ]
    }
   ],
   "source": [
    "for col in tabular_model.config.categorical_cols:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = \"missing\"\n",
    "        print('The column ',col,' is missing')# or a safe fallback category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a764761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      Direction_0_probability  Direction_1_probability  Direction_prediction\n",
      "0                         1.0             3.900224e-38                     0\n",
      "1                         1.0             2.361974e-21                     0\n",
      "2                         1.0             0.000000e+00                     0\n",
      "3                         1.0             0.000000e+00                     0\n",
      "4                         1.0             0.000000e+00                     0\n",
      "...                       ...                      ...                   ...\n",
      "1117                      1.0             0.000000e+00                     0\n",
      "1118                      1.0             0.000000e+00                     0\n",
      "1119                      0.0             1.000000e+00                     1\n",
      "1120                      1.0             0.000000e+00                     0\n",
      "1121                      1.0             0.000000e+00                     0\n",
      "\n",
      "[1122 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(type(preds))\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea5631c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Attributions are not implemented for TabTransformerModel",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtabular_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIntegratedGradients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vasistha\\anaconda3\\envs\\textmining\\lib\\site-packages\\pytorch_tabular\\tabular_model.py:2051\u001b[0m, in \u001b[0;36mTabularModel.explain\u001b[1;34m(self, data, method, method_args, baselines, **kwargs)\u001b[0m\n\u001b[0;32m   2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaselines cannot be a scalar or None for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a tensor or a string like `b|<num_samples>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2049\u001b[0m     )\n\u001b[0;32m   2050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_not_supported:\n\u001b[1;32m-> 2051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttributions are not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_get_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2053\u001b[0m is_embedding1d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membedding_layer, (Embedding1dLayer, PreEncoded1dLayer))\n\u001b[0;32m   2054\u001b[0m is_embedding2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membedding_layer, Embedding2dLayer)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Attributions are not implemented for TabTransformerModel"
     ]
    }
   ],
   "source": [
    "tabular_model.explain(test_df, method=\"IntegratedGradients\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5e5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
